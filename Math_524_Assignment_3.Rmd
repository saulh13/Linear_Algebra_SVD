---
title: "Math_524_Assignment_3"
author: "Saul Hernandez"
date: "2024-12-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 3.1

Use the K-means method to make a cluster analysis for the following five points in the 2D space: $P_1(1, 1), P_2(2, 2), P_3(2, 3), P_4(3, 4), P_5(4, 4)$. Assume $K = 2$. Plot a figure similar to Fig. 9.1. What is the final tWCSS equal to?

```{r}
# Define the data
N = 5
K = 2 
mydata = matrix(c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4),
                nrow = N, byrow = TRUE)
x1 = mydata[1, ]
x2 = mydata[2, ]
x3 = mydata[3, ]
x4 = mydata[4, ]
x5 = mydata[5, ]
```

```{r}
plot(0,  xlim = c(0, 5), ylim = c(0, 5), xlab = "x", ylab = "y")
points(x1[1], x1[2], col = 'red', pch = 16)
points(x2[1], x2[2], col = 'red', pch = 16)
points(x3[1], x3[2], col = 'red', pch = 16)
points(x4[1], x4[2], col = 'red', pch = 16)
points(x5[1], x5[2], col = 'red', pch = 16)
```

```{r}
# Perform k-means clustering (K = 2)
Kclusters = kmeans(mydata, K)

# Case 1: C1 = (P1, P2) and C2 = (P3, P4, P5)
c1 = (mydata[1, ] + mydata[2, ]) / 2  
c2 = (mydata[3, ] + mydata[4, ] + mydata[5, ]) / 3  

# Calculate tWCSS for Case 1
tWCSS_1 = norm(x1 - c1, type = '2')^2 + 
          norm(x2 - c1, type = '2')^2 + 
          norm(x3 - c2, type = '2')^2 + 
          norm(x4 - c2, type = '2')^2 + 
          norm(x5 - c2, type = '2')^2

cat("tWCSS for Case 1:", tWCSS_1, "\n")
```

```{r}
# Case 2: C1 = (P1, P3) and C2 = (P2, P4, P5)
c1 = (mydata[1, ] + mydata[3, ]) / 2
c2 = (mydata[2, ] + mydata[4, ] + mydata[5, ]) / 3
#tWCSS for Case 2
tWCSS_2 = norm(x1 - c1, type = '2')^2 + 
          norm(x3 - c1, type = '2')^2 + 
          norm(x2 - c2, type = '2')^2 + 
          norm(x4 - c2, type = '2')^2 + 
          norm(x5 - c2, type = '2')^2
cat("tWCSS for Case 2:", tWCSS_2, "\n")
```

```{r}
# Case 3: C1 = (P2, P3) and C2 = (P1, P4, P5)
c1 = (mydata[2, ] + mydata[3, ]) / 2  
c2 = (mydata[1, ] + mydata[4, ] + mydata[5, ]) / 3
#tWCSS for Case 3
tWCSS_3 = norm(x2 - c1, type = '2')^2 + 
          norm(x3 - c1, type = '2')^2 + 
          norm(x1 - c2, type = '2')^2 + 
          norm(x4 - c2, type = '2')^2 + 
          norm(x5 - c2, type = '2')^2
cat("tWCSS for Case 3:", tWCSS_3, "\n")
```

```{r}
#Clustering vector
Kclusters$cluster
#Cluster centers
Kclusters$centers
#Totss
Kclusters$totss
#tot.withinss
Kclusters$tot.withinss
#withinss
Kclusters$withinss
```

```{r}
# Create the plot
par(mar = c(5, 5, 2.5, 0.5))
plot(mydata[, 1], mydata[, 2], 
     col = Kclusters$cluster + 1,
     pch = 16, cex = 1.5, 
     xlim = c(0, 5), ylim = c(0, 5), 
     xlab = 'x', ylab = 'y', 
     main = 'K-means Clustering for 5 Points and 2 Clusters')

points(Kclusters$centers[, 1], Kclusters$centers[, 2], 
       col = c('red', 'skyblue'), pch = 4, cex = 2)

text(1.7, 1.6, bquote(C[1]), col = 'red', cex = 1.4)
text(3.5, 3.5, bquote(C[2]), col = 'skyblue', cex = 1.4)
text(1, 1.3, bquote(P[1]), cex = 1.4, col = 'red')
text(2, 1.3, bquote(P[2]), cex = 1.4, col = 'red')
text(2, 3.3, bquote(P[3]), cex = 1.4, col = 'red')
text(3, 4.2, bquote(P[4]), cex = 1.4, col = 'skyblue')
text(4, 4.2, bquote(P[5]), cex = 1.4, col = 'skyblue')
```

# Problem 3.3

Use the K-means method to make a cluster analysis for the daily TMIN and WDF2 data of the Miami International Airport in 2015, following the method presented in Sub-sections 9.1.3. Here, TMIN is the daily minimum temper4331 ature, and WDF2 denotes the direction [in degrees] of the fastest 2-minute wind in a day. You can obtain the data from the NOAA Climate Data Online website, or from the file
`MiamiIntlAirport2001_2020.csv` .

```{r}
setwd("~/Desktop/LinAlg")
dat = read.csv("data/MiamiIntlAirport2001_2020.csv", header = TRUE)
dat2015 = dat[5115:5479, ] #Year 2015
dim(dat2015)

# Extract Tmin and WDF2 columns
tmin2015 = dat2015[,'TMIN']
wdf2_2015 = dat2015[,'WDF2']

# Scatter plot
par(mar = c(4.5, 4.5, 2, 4.5))
plot(tmin2015, wdf2_2015, 
     pch = 16, cex = 0.5,
     xlab = 'Tmin [deg C]', 
     ylab = 'Wind Direction [deg]',
     main = '(a) 2015 Daily Miami Tmin vs WDF2')
axis(4, at = c(0, 45, 90, 135, 180, 225, 270, 315, 360), 
     lab = c('N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N'))
text('Wind Direction', side = 4, line = 3)
```

```{r}
#Tmin over the year
plot(tmin2015, 
     col = "blue", pch = 16, 
     xlab = "Day (Year 2015)", 
     ylab = "Tmin [deg C]",
     main = "2015 Daily Tmin at Miami Intl Airport")
```

```{r}
#WDF2 over the year
plot(wdf2_2015, type = "o", 
     col = "red", pch = 16, 
     xlab = "Day of Year (2015)", 
     ylab = "Wind Direction [deg]",
     main = "2015 Daily Wind Direction at Miami Intl Airport")
```

```{r}
# K-means clustering for 2015 data
K = 2 
mydata2015 = cbind(tmin2015, wdf2_2015) 
fit2015 = kmeans(mydata2015, K) 


fit2015$centers  # Cluster centers
fit2015$tot.withinss  # Total within-cluster

mycluster <- data.frame(mydata2015, fit2015$cluster)
names(mycluster)<-c('Tmin [deg C]', 
                    'Wind Direction [deg]',
                    'Cluster')
```

```{r}
library(animation)
par(mar = c(4.5, 4.5, 2, 4.5))  # Adjust margins for the plot
kmeans.ani(mycluster, centers = K, pch = 1:K, col = 1:K, hints = '')
title(main = "(b) K-means Clusters for Daily Tmin vs WDF2 (2015)", 
      cex.main = 0.8)
axis(4, at = c(0, 45, 90, 135, 180, 225, 270, 315, 360), 
     lab = c('N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N'))
mtext('Wind Direction', side = 4, line = 3)
```

# Problem 3.8

Following the method presented in Sub-section 9.2.1., make an SVM analysis for the following five points in a 2D space: $P_1(1, 1), P_2(2, 2), P_3(2, 3), P_4(3, 4), P5_(4, 4)$. The first three points are labeled 1 and the last two are labeled 2. What are $w$, $b$ and $D_m$? What points are the support vectors?

```{r}
x = matrix(c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4), ncol = 2, byrow = TRUE)  # Points
y = c(1, 1, 1, -1, -1)  # Labels for two categories (1 and -1)
```

```{r}
library(e1071)

dat = data.frame(x, y = as.factor(y))
svm_5p = svm(y ~ ., data = dat, 
             kernel = "linear", cost = 10, 
             scale = FALSE, type = "C-classification")
svm_5p
```

```{r}
# Find hyperplane, normal vector, and SV (wx + b = 0)
w = t(svm_5p$coefs) %*% svm_5p$SV 
b = svm_5p$rho
Dm = 2 / norm(w, type = '1')
```

```{r}
#parameters
cat("w = ", w, "\n")
cat("b =", b, "\n")
cat("Dm =", Dm, "\n")
```

```{r}
x1 = seq(0, 6, len = 100)
x2 = (b - w[1]*x1) / w[2] 
x2p = (1 + b - w[1]*x1) / w[2]  
x2m = (-1 + b - w[1]*x1) / w[2] 
x20 = (b - w[1]*x1)/w[2]
```

```{r}
par(mar = c(4.5, 4.5, 2.0, 2.0))
plot(x, col = y + 3, pch = 19,
     xlim = c(0, 5), ylim = c(0, 6),
     xlab = bquote(x[1]), ylab = bquote(x[2]),
     cex.lab = 1.5, cex.axis = 1.5,
     main = 'SVM for Five points labeled in two categories')
axis(2, at = (-2):8, tck = 1, lty = 2, 
     col = "grey", labels = NA)
axis(1, at = (-2):8, tck = 1, lty = 2, 
     col = "grey", labels = NA)
lines(x1, x2p, lty = 2, col = 4)
lines(x1, x2m, lty = 2, col = 2)
lines(x1, x20, lwd = 1.5, col = 'purple')
xnew = matrix(c(1.5, 1, 3, 3), ncol = 2, byrow = TRUE)
points(xnew, pch = 18, cex = 2)
for (i in 1:2) {
  text(xnew[i,1] + 0.5, xnew[i,2], paste("Q", i), 
       cex = 1.5, col = 6 - 2*i)
}
text(3, 5.8, "Training data are 3 blue points and two red point for SVM", cex = 0.7, col = 4)
text(3.55, 5, "Two Black diamond points are to be predicted by the SVM", cex = 0.7)
```

```{r}
#Predictions
predictions = predict(svm_5p, xnew)
predictions
```

# Problem 3.9

Two new points are introduced to the previous problem: $Q_1(1.5, 1)$ and $Q_2(3, 3)$. Use the SVM trained in the previous problem to find out which point belongs to what category.

```{r}
#New points Q1 and Q2
xnew = matrix(c(1.5, 1, 3, 3), ncol = 2, byrow = TRUE)

# Predict the categories
predictions = predict(svm_5p, xnew)

# Predictions
cat("Predictions for new points:\n")
cat("Q1(1.5, 1) belongs to:", predictions[1], "\n")
cat("Q2(3, 3) belongs to:", predictions[2], "\n")
```

# Problem 3.13

The first 50 in the 150 rows of the R.A. Fisher iris data are for setosa, 51-100 are for versocolor, and 101-150 are for virginica, use the data 1-40, 51-90, and 101-140 to train an RF model, following the method in Sub-Section 9.3.1. Use the RF model to predict the species of the remaining data of lengths and widths of petal and sepal. You can download the R.A. Fisher dataset `iris.csv` from the book website or use the data already built in R or Python software packages.

```{r}
library(randomForest)
data(iris)

# Rows 1-40 (Setosa), 51-90 (Versicolor), 101-140 (Virginica)
train_data = iris[c(1:40, 51:90, 101:140), ]

#Rows: 41-50 (Setosa), 91-100 (Versicolor), 141-150 (Virginica)
test_data = iris[c(41:50, 91:100, 141:150), ] 

# Train a Random Forest model
classifyRF = randomForest(x = train_data[, 1:4],  # Features
                          y = train_data[, 5],    # Species
                          ntree = 800)            # 800 trees
classifyRF
```

```{r}
plot(classifyRF, 
     main='RF model error rate for each tree') 
errRate = classifyRF$err.rate  # Extract the error rate
# dim(errRate)  
```

```{r}
# Plot error rates vs number of trees
tree_num = 1:800
plot(tree_num, errRate[,1], 
     type = 's', col='black',
     ylim = c(0, 0.2),
     xlab = 'Trees', ylab = 'Error rate', 
     main = 'RF model error rate for each tree',
     cex.axis = 1.3, cex.lab = 1.3)
lines(tree_num, errRate[,2], lwd = 1.8, lty = 2, type = 's', col='red')
lines(tree_num, errRate[,3], lwd = 1.8, lty = 3, type = 's', col='green')
lines(tree_num, errRate[,4], lwd = 1.8, lty = 4, type = 's', col='skyblue')

legend(400, 0.21, lwd = 2.5,
       legend = c('OOB','Setosa', 'Versicolor', 'Virginica'),
       col=c('black', 'red', 'green', 'skyblue'),
       lty=1:4, cex=1, y.intersp = 0.75, text.font = 2, box.lty=0)
```

```{r}
classifyRF$importance  # Show feature importance
```

```{r}
varImpPlot(classifyRF, sort = FALSE,
           lwd = 1.5, pch = 16, 
           main = 'Importance plot of the RF model',
           cex.axis = 1.3, cex.lab = 1.3)
```

```{r}
# Confusion
classifyRF$confusion

# RF prediction for the new data based on the trained trees
predictions = predict(classifyRF, newdata = test_data[, 1:4])
predictions
```

```{r}
# Another version of the randomForest()
anotherRF = randomForest(Species ~ ., data = train_data, ntree = 500)  # Alternative model
anotherRF
```

# Problem 3.14

For the 150 rows R.A. Fisher iris data, use only 20% of the data from each species to train your RF model. Select another 10% of the riris data of lengths and widthes as the new data for prediction. Then use the RF model to predict the species of the new data. Discuss the errors of your RF model and your prediction.

```{r}
train_data = data.frame()
test_data = data.frame()

# Process each species separately
for (species in unique(iris$Species)) {
  species_data = iris[iris$Species == species, ]
 
   # Calculate rows
  train_size = floor(0.20 * nrow(species_data))  # 20% for training
  test_size = floor(0.10 * nrow(species_data))   # 10% for testing
  
  #Random Sample 
  train_indices = sample(1:nrow(species_data), train_size)
  test_indices = setdiff(1:nrow(species_data), train_indices)
  
  #Join Everything
  test_indices = sample(test_indices, test_size) 
  train_data = rbind(train_data, species_data[train_indices, ])
  test_data = rbind(test_data, species_data[test_indices, ])
}
```

```{r}
#Random Forest Model
classifyRF = randomForest(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, 
                          data = train_data, 
                          ntree = 800) 
classifyRF
```

```{r}
# Plot the error rate
plot(classifyRF, main = 'RF model error rate for each tree')
errRate = classifyRF$err.rate 
```

```{r}
tree_num = 1:800
plot(tree_num, errRate[, 1], type = 's', col = 'black', ylim = c(0, 0.2),
     xlab = 'Trees', ylab = 'Error rate', main = 'RF model error rate for each tree',
     cex.axis = 1.3, cex.lab = 1.3)
lines(tree_num, errRate[, 2], lwd = 1.8, lty = 2, type = 's', col = 'red')
lines(tree_num, errRate[, 3], lwd = 1.8, lty = 3, type = 's', col = 'green')
lines(tree_num, errRate[, 4], lwd = 1.8, lty = 4, type = 's', col = 'skyblue')

legend(400, 0.21, lwd = 2.5,
       legend = c('OOB', 'Setosa', 'Versicolor', 'Virginica'),
       col = c('black', 'red', 'green', 'skyblue'),
       lty = 1:4, cex = 1.2, y.intersp = 0.6, text.font = 1, box.lty = 0)
```

```{r}
 # Variable importance
print(classifyRF$importance) 
```

```{r}
varImpPlot(classifyRF, sort = FALSE,
           lwd = 1.5, pch = 16,
           main = 'Importance plot of the RF model',
           cex.axis = 1.3, cex.lab = 1.3)
```

```{r}
classifyRF$confusion

predictions = predict(classifyRF, newdata = test_data[, 1:4])
predictions
```

# Problem 3.19

The first 50 in the 150 rows of the R.A. Fisher iris data are for setosa, 51-100 are for versocolor, and 101 are for virginica, use the data 1-40, 51-90, and 101-140 to train an NN model, following the method in Sub-Section 9.4.2. Use the NN model to predict the species of the remaining data of lengths and widths of petal and sepal.

```{r}
library(neuralnet)

train_data = iris[c(1:40, 51:90, 101:140), ]  
test_data = iris[c(41:50, 91:100, 141:150), ]

train_data$Setosa = ifelse(train_data$Species == "setosa", 1, 0)
train_data$Versicolor = ifelse(train_data$Species == "versicolor", 1, 0)
train_data$Virginica = ifelse(train_data$Species == "virginica", 1, 0)
```

```{r}
set.seed(123)
nn = neuralnet(Setosa + Versicolor + Virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = train_data, hidden = 5, act.fct = "logistic", linear.output = FALSE)

# Plot the neural network
plot(nn)
```

```{r}
test = test_data[, c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")]
Predict = neuralnet::compute(nn, test)

ifelse(Predict$net.result > 0.5, 1, 0)
```

```{r}
# NN model parameters
#Weights
nn$weights
```

```{r}
#print error and other technical indices of the nn run
nn$result.matrix #results data
```

# Problem 3.20

For the 150 rows R.A. Fisher iris data, use only 20% of the data from each species to train your NN model. Select another 10% of the riris data of lengths and widthes as the new data for prediction. Then use the NN model to predict the species of the new data. Discuss the errors of your NN model and your prediction.

```{r}
library(neuralnet)


##SAME AS 3.14
train_data = data.frame()
test_data = data.frame()

# Process each species separately
for (species in unique(iris$Species)) {
  # Subset data for the current species
  species_data = iris[iris$Species == species, ]
  
  train_size = floor(0.20 * nrow(species_data))
  test_size = floor(0.10 * nrow(species_data))
  
  train_indices = sample(1:nrow(species_data), train_size)
  test_indices = setdiff(1:nrow(species_data), train_indices)
  test_indices = sample(test_indices, test_size)

  train_data = rbind(train_data, species_data[train_indices, ])
  test_data = rbind(test_data, species_data[test_indices, ])
}

```

```{r}
train_data$Setosa = ifelse(train_data$Species == "setosa", 1, 0)
train_data$Versicolor = ifelse(train_data$Species == "versicolor", 1, 0)
train_data$Virginica = ifelse(train_data$Species == "virginica", 1, 0)
```

```{r}
set.seed(123)
nn = neuralnet(Setosa + Versicolor + Virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
               data = train_data, hidden = 5, act.fct = "logistic", linear.output = FALSE)

plot(nn)
```

```{r}
test = test_data[, c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")]
Predict = neuralnet::compute(nn, test)

ifelse(Predict$net.result > 0.5, 1, 0)
```

```{r}
# NN model parameters
#Weights
nn$weights
```

```{r}
#print error and other technical indices of the nn run
nn$result.matrix #results data
```
